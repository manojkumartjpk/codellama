import json
import numpy as np
import torch
from transformers import pipeline, AutoTokenizer
import transformers


class InferlessPythonModel:

    # replace ##task_type## and ##huggingface_name## with appropriate values
    def initialize(self):
        self.tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-hf")

        self.generator = pipeline(
            "text-generation",
            model="codellama/CodeLlama-7b-hf",
            torch_dtype=torch.float16,
            device=0,
        )

    # inputs is a dictonary where the keys are input names and values are actual input data
    # e.g. in the below code the input name is prompt
    # The output generated by the infer function should be a dictonary where keys are output names and values are actual output data
    # e.g. in the below code the output name is generated_txt
    def infer(self, inputs):
        prompt = inputs["prompt"]
        sequences = self.generator(
            prompt,
            do_sample=True,
            top_k=10,
            temperature=0.1,
            top_p=0.95,
            num_return_sequences=1,
            eos_token_id=self.tokenizer.eos_token_id,
            max_length=200,
        )
        generated_txt = ""
        for seq in sequences:
            generated_txt = seq["generated_text"] + "\n"
        return {"generated_text": generated_txt}

    # perform any cleanup activity here
    def finalize(self, args):
        self.pipe = None
